{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense \n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('.', 'data', 'final', 'public')\n",
    "train_values = pd.read_csv(DATA_DIR / 'train_values.csv', index_col='building_id')\n",
    "train_labels = pd.read_csv(DATA_DIR / 'train_labels.csv', index_col='building_id')\n",
    "test_values = pd.read_csv(DATA_DIR / 'test_values.csv', index_col='building_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "del presotere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class presotere(TransformerMixin):\n",
    "    \n",
    "    def __init__(self,caso):\n",
    "        self.lencoder_col=list([])\n",
    "        self.object_cols=list([])\n",
    "        self.number_cols = list([])\n",
    "        self.lencoder_col=list([])\n",
    "        self._caso=caso\n",
    "        self.legeo = LabelEncoder()\n",
    "        self.lencoder= list([])\n",
    "        self._sns_data=None\n",
    "        self._y=None\n",
    "        self._numeric_transformer =[\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())]     \n",
    "        self._categorical_transformer = [\n",
    "            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore'))]       \n",
    "        \n",
    "    def inicializamodelo(self,caso):\n",
    "\n",
    "        if caso == 1:\n",
    "            self.legeo = LabelEncoder()\n",
    "            self.lencoder= list([])\n",
    "            print(\"Inicializa LabelEncoder caso 1\")\n",
    "        if caso == 2:\n",
    "            self.legeo = LabelEncoder()\n",
    "            self.lencoder= OneHotEncoder(handle_unknown='ignore', sparse=False)   \n",
    "            print(\"Inicializa OneHotEncoder caso 2\")\n",
    "        \n",
    "        \n",
    "    def transform(self,X,y=None, **kwargs): \n",
    "        if self._caso == 1:\n",
    "            contador=0\n",
    "            self._sns_data=X[self.number_cols].copy()\n",
    "            for col in X[self.object_cols].columns:\n",
    "                self.lencoder.append(LabelEncoder())\n",
    "                self.lencoder[contador].fit(X[col])\n",
    "                self._sns_data[col]=self.lencoder[contador].transform(X[col])  \n",
    "                contador=contador+1\n",
    "                \n",
    "                \n",
    "        if self._caso == 2:  \n",
    "            print(\"Transforma caso 2\")\n",
    "            nada = None\n",
    "            self._sns_data=None\n",
    "            print(\"Transforma caso 2.1\")\n",
    "            nada = self.lencoder.fit_transform(X[self.object_cols])\n",
    "            co1c=0\n",
    "            self.lencoder_col=list([])\n",
    "            print(\"Transforma caso 2.2\")\n",
    "            for co1 in self.lencoder.categories_:\n",
    "                for co2 in co1:\n",
    "                    self.lencoder_col.append(self.object_cols[co1c]+\"_\"+co2)\n",
    "                co1c=co1c+1\n",
    "            print(\"Transforma caso 2.3\",nada.shape,len(self.lencoder_col)) \n",
    "            objedf=pd.DataFrame(nada,columns=self.lencoder_col,\n",
    "                                         index=X[self.object_cols].index.tolist())\n",
    "            print(\"Transforma caso 2.4\",objedf.shape) \n",
    "            self._sns_data=pd.concat([X[self.number_cols].copy(),objedf],axis=1)  \n",
    "            print(\"NEcoder\",self._sns_data.shape)\n",
    "#            self._sns_data.to_csv(DATA_DIR / 'self._sns_data_05.csv')            \n",
    "#            self._sns_data=self._sns_data.drop(self.object_cols,axis=1)\n",
    "                \n",
    "                \n",
    "        geo_level_1_fact=math.pow(10,int(math.log(self._sns_data['geo_level_2_id'].max(),10)+1))\n",
    "        geo_level_2_fact=math.pow(10,int(math.log(self._sns_data['geo_level_3_id'].max(),10)+1))\n",
    "        self._sns_data['geo_level_n']=  self._sns_data['geo_level_1_id']*geo_level_1_fact*geo_level_2_fact+self._sns_data['geo_level_2_id']*geo_level_2_fact+self._sns_data['geo_level_3_id']\n",
    "        self._sns_data['geo_level']=self._sns_data['geo_level_n']#.astype(np.int64).astype(str)\n",
    "        self._sns_data['geo_level_2']=self._sns_data['geo_level_n']*self._sns_data['geo_level_n']\n",
    "        self._sns_data['geo_level_cod']=self._sns_data['geo_level_n'].astype(np.int64)\n",
    "\n",
    "        self._sns_data=self._sns_data.drop(['geo_level_n','geo_level'],axis=1)\n",
    "\n",
    "        self._sns_data=self._sns_data.drop(['count_floors_pre_eq'],axis=1)\n",
    "        min_max_scaler = MinMaxScaler()\n",
    "        self._sns_data[list(self._sns_data.columns)]=min_max_scaler.fit_transform(self._sns_data.values)\n",
    "        lcolum_x = list(self._sns_data.columns) \n",
    "        print(\"NEcoder 1\",self._sns_data.shape)\n",
    "        print('Fin transform')\n",
    "        return self._sns_data\n",
    "    \n",
    "    def fit(self,X, y=None, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            if key==\"caso\":\n",
    "                self._caso=value\n",
    "        s = (X.dtypes == 'object')\n",
    "        self.object_cols = list(s[s].index)\n",
    "        s = (X.dtypes != 'object')        \n",
    "        self.number_cols = list(s[s].index) \n",
    "        self._y=y\n",
    "        self.inicializamodelo(self._caso)\n",
    "        self.transform(X,y, **kwargs)\n",
    "        print('Fin fit')\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return(self._y)\n",
    "\n",
    "    def fit_transform(self,X, y=None, **kwargs):\n",
    "            miX=X.values\n",
    "            miX=miX.reshape(-1, 1)\n",
    "            self.fit(X, y, **kwargs)\n",
    "            nada=self.transform(X, **kwargs)\n",
    "            print('Fin fit_transform')\n",
    "            return(nada)\n",
    "        \n",
    "    def set_params(self,**kwargs):\n",
    "            for key, value in kwargs.items():\n",
    "                if key==\"caso\":\n",
    "                    self._caso=value\n",
    "                    print(key,self._caso)\n",
    "            return self        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargamodelo(modelcaso):\n",
    "    global model\n",
    "    if modelcaso==1:\n",
    "        model = LogisticRegression(random_state=0, solver='lbfgs',multi_class='multinomial',max_iter=500)\n",
    "    if modelcaso==2:    \n",
    "        model = RandomForestRegressor(max_depth=20, random_state=0,n_estimators=100)\n",
    "    if modelcaso==3:\n",
    "        model = DecisionTreeClassifier(random_state=0,max_depth=20) \n",
    "    if modelcaso==4:\n",
    "        model = MultinomialNB\n",
    "    if modelcaso==5:\n",
    "        model = DecisionTreeRegressor(random_state=0)   \n",
    "    if modelcaso==6:\n",
    "        model = SVC(gamma='auto',verbose=True,kernel='linear', probability=True)          \n",
    "    if modelcaso==9:    \n",
    "        model = Sequential()\n",
    "        model.add(Dense(12, input_dim=36, activation='relu'))\n",
    "        model.add(Dense(30, activation='relu'))\n",
    "        model.add(Dense(15, activation='relu'))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'steps' in globals():\n",
    "    del steps\n",
    "if 'pipeline' in globals():    \n",
    "    del pipeline\n",
    "steps = [('Preprosessor', presotere(caso=1)),('Modelo',RandomForestRegressor(max_depth=20, random_state=0,n_estimators=100))]\n",
    "#steps = [('Preprosessor', presotere(caso=1))]\n",
    "pipeline = Pipeline(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameteres = {'Preprosessor__caso':[1,2],'Modelo__max_depth':[10,20]}\n",
    "#grid = GridSearchCV(pipeline, param_grid=parameteres, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicializa OneHotEncoder caso 2\n",
      "Transforma caso 2\n",
      "Transforma caso 2.1\n",
      "Transforma caso 2.2\n",
      "Transforma caso 2.3 (260601, 38) 38\n",
      "Transforma caso 2.4 (260601, 38)\n",
      "NEcoder (260601, 68)\n",
      "NEcoder 1 (260601, 69)\n",
      "Fin transform\n",
      "Fin fit\n",
      "Transforma caso 2\n",
      "Transforma caso 2.1\n",
      "Transforma caso 2.2\n",
      "Transforma caso 2.3 (260601, 38) 38\n",
      "Transforma caso 2.4 (260601, 38)\n",
      "NEcoder (260601, 68)\n",
      "NEcoder 1 (260601, 69)\n",
      "Fin transform\n",
      "Fin fit_transform\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\program files\\python37\\lib\\site-packages\\sklearn\\pipeline.py:267: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  self._final_estimator.fit(Xt, y, **fit_params)\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(train_values,train_labels,Preprosessor__caso=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(train_values,train_labels,scoring=\"f1_micro\" ,n_jobs=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(train_values,train_labels)\n",
    "nada=pipeline.predict(train_values)\n",
    "test_data=pipeline.predict(test_values)\n",
    "pvalues=test_values[['geo_level_1_id']]\n",
    "pvalues['damage_grade']=test_data.round().astype(np.int64)\n",
    "pvalues=pvalues.drop(['geo_level_1_id'],axis=1)\n",
    "pvalues.to_csv(DATA_DIR / 'submission_00_05.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(260601, 38)\n"
     ]
    }
   ],
   "source": [
    "print(train_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
